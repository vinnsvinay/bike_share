{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "\n",
    "INPUT_DATA = '../data/sf-bay-area-bike-share/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "station_id,bikes_available,docks_available,time\r\n",
      "2,2,25,2013/08/29 12:06:01\r\n",
      "2,2,25,2013/08/29 12:07:01\r\n",
      "2,2,25,2013/08/29 12:08:01\r\n",
      "2,2,25,2013/08/29 12:09:01\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 {INPUT_DATA}status.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,name,lat,long,dock_count,city,installation_date\r\n",
      "2,San Jose Diridon Caltrain Station,37.329732,-121.90178200000001,27,San Jose,8/6/2013\r\n",
      "3,San Jose Civic Center,37.330698,-121.888979,15,San Jose,8/5/2013\r\n",
      "4,Santa Clara at Almaden,37.333988,-121.894902,11,San Jose,8/6/2013\r\n",
      "5,Adobe on Almaden,37.331415,-121.8932,19,San Jose,8/5/2013\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 {INPUT_DATA}station.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_status = sc.textFile(INPUT_DATA + 'status.csv').cache()\n",
    "header = bike_status.first()\n",
    "bike_status = bike_status.filter(lambda x: x != header).map(lambda x: x.split(',')).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71984434"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike_status.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2013, 8, 29, 12, 6, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ts = bike_status.take(1)[-1][-1]\n",
    "dt = datetime.strptime(test_ts, \"%Y/%m/%d %H:%M:%S\")\n",
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toIntSafe(num):\n",
    "    try:\n",
    "        return int(num)\n",
    "    except ValueError:\n",
    "        return None\n",
    "    \n",
    "def toFloatSafe(num):\n",
    "    try:\n",
    "        return float(num)\n",
    "    except ValueError:\n",
    "        return None    \n",
    "    \n",
    "def toTimeStampSafe(data):\n",
    "    try:\n",
    "        return datetime.strptime(data, \"%Y/%m/%d %H:%M:%S\") \n",
    "    except ValueError:\n",
    "        return None\n",
    "    \n",
    "def convertData(data):\n",
    "    return (toIntSafe(data[0]),\n",
    "            toIntSafe(data[1]),\n",
    "            toIntSafe(data[2]),\n",
    "            toTimeStampSafe(data[3]))\n",
    "\n",
    "def preprocess_data(data):\n",
    "    try:\n",
    "        return (data[0], (data[2]*1.0)/(data[1] + data[2]), data[3].year,\n",
    "            data[3].month, data[3].day, data[3].hour, data[3].minute, data[3].isoweekday())\n",
    "    except (AttributeError, ValueError, ZeroDivisionError):\n",
    "        return None\n",
    "\n",
    "bike_status_processed = bike_status.map(lambda x:\n",
    "                                        convertData(x)).map(lambda x:preprocess_data(x)).filter(lambda x: x != None).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16994602"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike_status_processed.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[3] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike_status.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 0.9259259259259259, 2013, 8, 29, 12, 6, 4),\n",
       " (2, 0.9259259259259259, 2013, 8, 29, 12, 7, 4),\n",
       " (2, 0.9259259259259259, 2013, 8, 29, 12, 8, 4)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike_status_processed.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_status_schema = StructType([StructField('station_id', IntegerType(),False),\n",
    "                                StructField('bikes_utilised_percentage', FloatType(),False),\n",
    "                                StructField('year', IntegerType(), False),\n",
    "                                StructField('month', IntegerType(), False),\n",
    "                                StructField('day', IntegerType(), False),\n",
    "                                StructField('hour', IntegerType(), False),\n",
    "                                StructField('minute', IntegerType(), False),\n",
    "                                StructField('day_of_week', IntegerType(), False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------------+----+-----+---+----+------+-----------+\n",
      "|station_id|bikes_utilised_percentage|year|month|day|hour|minute|day_of_week|\n",
      "+----------+-------------------------+----+-----+---+----+------+-----------+\n",
      "|         2|                0.9259259|2013|    8| 29|  12|     6|          4|\n",
      "|         2|                0.9259259|2013|    8| 29|  12|     7|          4|\n",
      "|         2|                0.9259259|2013|    8| 29|  12|     8|          4|\n",
      "|         2|                0.9259259|2013|    8| 29|  12|     9|          4|\n",
      "|         2|                0.9259259|2013|    8| 29|  12|    10|          4|\n",
      "+----------+-------------------------+----+-----+---+----+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bike_status_df = sqlContext.createDataFrame(bike_status_processed, bike_status_schema).cache()\n",
    "bike_status_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16994602"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike_status_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_id: integer (nullable = false)\n",
      " |-- bikes_utilised_percentage: float (nullable = false)\n",
      " |-- year: integer (nullable = false)\n",
      " |-- month: integer (nullable = false)\n",
      " |-- day: integer (nullable = false)\n",
      " |-- hour: integer (nullable = false)\n",
      " |-- minute: integer (nullable = false)\n",
      " |-- day_of_week: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bike_status_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[6] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike_status_processed.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(day_of_week=1),\n",
       " Row(day_of_week=6),\n",
       " Row(day_of_week=3),\n",
       " Row(day_of_week=5),\n",
       " Row(day_of_week=4),\n",
       " Row(day_of_week=7),\n",
       " Row(day_of_week=2)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike_status_df.select('day_of_week').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hour=12),\n",
       " Row(hour=22),\n",
       " Row(hour=1),\n",
       " Row(hour=13),\n",
       " Row(hour=16),\n",
       " Row(hour=6),\n",
       " Row(hour=3),\n",
       " Row(hour=20),\n",
       " Row(hour=5),\n",
       " Row(hour=19),\n",
       " Row(hour=15),\n",
       " Row(hour=17),\n",
       " Row(hour=9),\n",
       " Row(hour=4),\n",
       " Row(hour=8),\n",
       " Row(hour=23),\n",
       " Row(hour=7),\n",
       " Row(hour=10),\n",
       " Row(hour=21),\n",
       " Row(hour=11),\n",
       " Row(hour=14),\n",
       " Row(hour=2),\n",
       " Row(hour=0),\n",
       " Row(hour=18)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike_status_df.select('hour').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------------+----+-----+---+----+------+-----------+---------+---------+\n",
      "|station_id|bikes_utilised_percentage|year|month|day|hour|minute|day_of_week| day_part|isWeekday|\n",
      "+----------+-------------------------+----+-----+---+----+------+-----------+---------+---------+\n",
      "|         2|                0.9259259|2013|    8| 29|  12|     6|          4|afternoon|        1|\n",
      "|         2|                0.9259259|2013|    8| 29|  12|     7|          4|afternoon|        1|\n",
      "|         2|                0.9259259|2013|    8| 29|  12|     8|          4|afternoon|        1|\n",
      "|         2|                0.9259259|2013|    8| 29|  12|     9|          4|afternoon|        1|\n",
      "|         2|                0.9259259|2013|    8| 29|  12|    10|          4|afternoon|        1|\n",
      "+----------+-------------------------+----+-----+---+----+------+-----------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "bike_status_period = bike_status_df.withColumn('day_part',\n",
    "                                               F.when((bike_status_df[\"hour\"] >= 20) | (bike_status_df[\"hour\"] < 6), 'night').\\\n",
    "                                               when((bike_status_df[\"hour\"] >= 6) & (bike_status_df[\"hour\"] < 12), 'morning').\\\n",
    "                                               when((bike_status_df[\"hour\"] >= 12) & (bike_status_df[\"hour\"] < 16), 'afternoon').\\\n",
    "                                               otherwise('evening'))\n",
    "\n",
    "bike_status_period = bike_status_period.withColumn('isWeekday',\n",
    "                                                   F.when(bike_status_period[\"day_of_week\"] <= 5, 1).otherwise(0)).cache()\n",
    "bike_status_period.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(day_part=u'afternoon'),\n",
       " Row(day_part=u'night'),\n",
       " Row(day_part=u'morning'),\n",
       " Row(day_part=u'evening')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike_status_period.select('day_part').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16994602"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike_status_period.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16994602"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike_status_period = bike_status_period.drop('year', 'hour', 'minute', 'day', 'isWeekday').cache()\n",
    "bike_status_period.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------+-----------+-------------+-------------+--------------------+-------------------+\n",
      "|station_id|month|day_part|day_of_week|min_util_perc|max_util_perc|       var_util_perc|      avg_util_perc|\n",
      "+----------+-----+--------+-----------+-------------+-------------+--------------------+-------------------+\n",
      "|         5|   10| evening|          4|   0.10526316|   0.68421054|0.007285435119161...| 0.5196670549274289|\n",
      "|         6|   12| morning|          6|   0.46666667|   0.73333335|0.004485562673316062| 0.6242195955995057|\n",
      "|         7|    9| morning|          1|   0.46666667|    0.6666667|0.005159173545484629| 0.5324195619312885|\n",
      "|         8|   12| morning|          2|          0.2|          0.6|0.011087554853032108|0.44103705240620505|\n",
      "|        10|    9| morning|          6|   0.33333334|          0.8|0.018026613717539944| 0.5929139699094639|\n",
      "+----------+-----+--------+-----------+-------------+-------------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bike_status_final = bike_status_period.groupBy('station_id', 'month', 'day_part', 'day_of_week')\\\n",
    "                        .agg(F.min('bikes_utilised_percentage').alias('min_util_perc'),\n",
    "                             F.max('bikes_utilised_percentage').alias('max_util_perc'),\n",
    "                             F.variance('bikes_utilised_percentage').alias('var_util_perc'),\n",
    "                             F.mean('bikes_utilised_percentage').alias('avg_util_perc'))\n",
    "bike_status_final.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------+-----------+--------------------+-------------------+\n",
      "|station_id|month|day_part|day_of_week|       var_util_perc|      avg_util_perc|\n",
      "+----------+-----+--------+-----------+--------------------+-------------------+\n",
      "|         5|   10| evening|          4|0.007285435119161...| 0.5196670549274289|\n",
      "|         6|   12| morning|          6|0.004485562673316062| 0.6242195955995057|\n",
      "|         7|    9| morning|          1|0.005159173545484629| 0.5324195619312885|\n",
      "|         8|   12| morning|          2|0.011087554853032108|0.44103705240620505|\n",
      "|        10|    9| morning|          6|0.018026613717539944| 0.5929139699094639|\n",
      "+----------+-----+--------+-----------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bike_status_final = bike_status_period.groupBy('station_id', 'month', 'day_part', 'day_of_week')\\\n",
    "                        .agg(F.variance('bikes_utilised_percentage').alias('var_util_perc'),\n",
    "                             F.mean('bikes_utilised_percentage').alias('avg_util_perc'))\n",
    "bike_status_final.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11720"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike_status_final.cache()\n",
    "bike_status_final.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,name,lat,long,dock_count,city,installation_date\r\n",
      "2,San Jose Diridon Caltrain Station,37.329732,-121.90178200000001,27,San Jose,8/6/2013\r\n",
      "3,San Jose Civic Center,37.330698,-121.888979,15,San Jose,8/5/2013\r\n",
      "4,Santa Clara at Almaden,37.333988,-121.894902,11,San Jose,8/6/2013\r\n",
      "5,Adobe on Almaden,37.331415,-121.8932,19,San Jose,8/5/2013\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 {INPUT_DATA}station.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------+-----------+--------------------+-------------------+\n",
      "|station_id|month|day_part|day_of_week|       var_util_perc|      avg_util_perc|\n",
      "+----------+-----+--------+-----------+--------------------+-------------------+\n",
      "|         5|   10| evening|          4|0.007285435119161...| 0.5196670549274289|\n",
      "|         6|   12| morning|          6|0.004485562673316062| 0.6242195955995057|\n",
      "|         7|    9| morning|          1|0.005159173545484629| 0.5324195619312885|\n",
      "|         8|   12| morning|          2|0.011087554853032108|0.44103705240620505|\n",
      "|        10|    9| morning|          6|0.018026613717539944| 0.5929139699094639|\n",
      "+----------+-----+--------+-----------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# bike_status_updated = bike_status_final.withColumn('station_id',\n",
    "#     F.when(bike_status_final['station_id'] == 80, 15).\n",
    "#     when(bike_status_final['station_id'] == 82, 78).\n",
    "#     when(bike_status_final['station_id'] == 83, 20).\n",
    "#     when(bike_status_final['station_id'] == 84, 17).\n",
    "#     otherwise(bike_status_final['station_id']))\n",
    "\n",
    "# bike_status_updated.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily_avg = bike_status_period.select('station_id', 'day_part', 'isWeekday', 'bikes_utilised_percentage').\\\n",
    "#                                    groupBy('station_id', 'day_part', 'isWeekday').\\\n",
    "#     mean('bikes_utilised_percentage').cache()\n",
    "\n",
    "# daily_avg.show(5)\n",
    "# daily_avg = daily_avg.withColumnRenamed('avg(bikes_utilised_percentage)', 'avg_bike_util')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# station = sc.textFile(INPUT_DATA + 'station.csv')\n",
    "# header = station.first()\n",
    "# station = station.filter(lambda x: x != header).map(lambda x: x.split(',')).map(lambda x:\n",
    "#                                                                                 (int(x[0]), float(x[2]),float(x[3])))\n",
    "\n",
    "# station_schema = StructType([StructField('station_id', IntegerType(), False),\n",
    "#                             StructField('latitude', DoubleType(), False),\n",
    "#                             StructField('longitude', DoubleType(), False)])\n",
    "\n",
    "# station_df = sqlContext.createDataFrame(station, station_schema)\n",
    "\n",
    "# bike_combined = bike_status_final.join(station_df, on='station_id')\n",
    "# bike_combined.show(5)\n",
    "\n",
    "# bike_combined = bike_combined.drop('station_id') \n",
    "# bike_combined.cache()\n",
    "# bike_combined.count()\n",
    "\n",
    "# bike_combined.show(5)\n",
    "\n",
    "# bike_combined.toPandas().to_csv(INPUT_DATA + 'bike_aggregated_data.csv', index = False)\n",
    "\n",
    "# !head -5 {INPUT_DATA}bike_aggregated_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "def implement_string_indexer(cols, df):\n",
    "    for c in cols:\n",
    "        si = StringIndexer(inputCol=c, outputCol=c+'_si')\n",
    "        sm = si.fit(df)\n",
    "        df = sm.transform(df).drop(c)\n",
    "        df = df.withColumnRenamed(c + '_si', c)\n",
    "        return df\n",
    "\n",
    "cols = ['day_part']\n",
    "\n",
    "final_df = implement_string_indexer(cols, bike_status_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------+--------------------+-------------------+--------+\n",
      "|station_id|month|day_of_week|       var_util_perc|      avg_util_perc|day_part|\n",
      "+----------+-----+-----------+--------------------+-------------------+--------+\n",
      "|         5|   10|          4|0.007285435119161...| 0.5196670549274289|     2.0|\n",
      "|         6|   12|          6|0.004485562673316062| 0.6242195955995057|     3.0|\n",
      "|         7|    9|          1|0.005159173545484629| 0.5324195619312885|     3.0|\n",
      "|         8|   12|          2|0.011087554853032108|0.44103705240620505|     3.0|\n",
      "|        10|    9|          6|0.018026613717539944| 0.5929139699094639|     3.0|\n",
      "+----------+-----+-----------+--------------------+-------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.cache()\n",
    "final_df.count()\n",
    "final_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_ids = final_df.select('station_id').collect()\n",
    "station_ids = [x.asDict()['station_id'] for x in station_ids]\n",
    "df_without_sid = final_df.drop('station_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[4.0,0.5196670549...|\n",
      "|[6.0,0.6242195955...|\n",
      "|[1.0,0.5324195619...|\n",
      "|[2.0,0.4410370524...|\n",
      "|[6.0,0.5929139699...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# input_cols = ['day_of_week', 'avg_util_perc', 'month', \n",
    "#               'day_part', 'latitude', 'longitude',\n",
    "#               'min_util_perc', 'max_util_perc', 'var_util_perc']\n",
    "\n",
    "input_cols = ['day_of_week', 'avg_util_perc', 'month', \n",
    "              'day_part', 'var_util_perc']\n",
    "\n",
    "va = VectorAssembler(inputCols= input_cols, outputCol= 'features')\n",
    "df_transformed = va.transform(df_without_sid).select('features')\n",
    "df_transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11720"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transformed.cache()\n",
    "df_transformed.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "#                         withStd=True, withMean=True)\n",
    "\n",
    "# # Compute summary statistics by fitting the StandardScaler\n",
    "# scalerModel = scaler.fit(df_transformed)\n",
    "\n",
    "# # Normalize each feature to have unit standard deviation.\n",
    "# scaledData = scalerModel.transform(df_transformed).select('scaledFeatures').\\\n",
    "#                 withColumnRenamed('scaledFeatures', 'features')\n",
    "    \n",
    "# scaledData.cache()\n",
    "# scaledData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_wssse = 1e10\n",
    "\n",
    "for clusters in range(2,20):\n",
    "\n",
    "    # Trains a k-means model.\n",
    "    kmeans = KMeans().setK(clusters).setSeed(1)\n",
    "    model = kmeans.fit(df_transformed)\n",
    "\n",
    "    # Evaluate clustering by computing Within Set Sum of Squared Errors.\n",
    "    wssse = model.computeCost(df_transformed)\n",
    "    if wssse <= min_wssse:\n",
    "        min_wssse = wssse\n",
    "        best_n_clusters = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Within Set Sum of Squared Errors = 12538.8344448\n",
      "Best number of clusters = 19\n",
      "Cluster Centers: \n",
      "[ 6.          0.52275087  8.5         2.5         0.01304861]\n",
      "[ 1.5         0.50786387  1.50364964  0.5         0.01453814]\n",
      "[  4.5          0.49685866  11.5          0.5          0.02099614]\n",
      "[ 6.2         0.51249939  1.50364964  2.6         0.01617827]\n",
      "[  2.06213592   0.49662004  11.13009709   0.43786408   0.01816505]\n",
      "[ 1.5         0.50770679  9.42857143  1.71428571  0.02375764]\n",
      "[ 3.5         0.51419353  1.50364964  0.5         0.01967513]\n",
      "[  6.5          0.50107223  11.           2.5          0.01742232]\n",
      "[ 4.          0.51138767  9.38461538  2.46153846  0.02545185]\n",
      "[ 4.28571429  0.50842867  9.          0.5         0.01719346]\n",
      "[ 5.16747573  0.51629141  1.66990291  1.16504854  0.01868887]\n",
      "[ 6.16625917  0.51049487  1.16870416  0.33251834  0.01665369]\n",
      "[ 7.          0.50773029  2.          0.5         0.01406227]\n",
      "[  4.5          0.50548268  11.5          2.5          0.02498372]\n",
      "[  6.5          0.50029545  11.5          0.5          0.01721704]\n",
      "[ 6.4         0.51349173  9.2         0.5         0.01898748]\n",
      "[ 1.5         0.52038322  1.50364964  2.5         0.0225847 ]\n",
      "[  2.           0.50878917  11.50387597   2.5          0.02301294]\n",
      "[ 3.5         0.52462049  1.50364964  2.5         0.02427115]\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Within Set Sum of Squared Errors = \" + str(min_wssse))\n",
    "print(\"Best number of clusters = \" + str(best_n_clusters))\n",
    "\n",
    "kmeans = KMeans().setK(best_n_clusters).setSeed(1)\n",
    "model = kmeans.fit(df_transformed)\n",
    "\n",
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform(df_transformed).select('prediction').collect()\n",
    "predicted_cluster = [x.asDict()['prediction'] for x in prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 512,\n",
       "         1: 548,\n",
       "         2: 512,\n",
       "         3: 685,\n",
       "         4: 1030,\n",
       "         5: 896,\n",
       "         6: 548,\n",
       "         7: 768,\n",
       "         8: 832,\n",
       "         9: 896,\n",
       "         10: 412,\n",
       "         11: 409,\n",
       "         12: 138,\n",
       "         13: 512,\n",
       "         14: 512,\n",
       "         15: 640,\n",
       "         16: 548,\n",
       "         17: 774,\n",
       "         18: 548})"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(predicted_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_id_rd =  sc.parallelize(zip(station_ids, predicted_cluster))\n",
    "\n",
    "station_id_schema = StructType([StructField('station_id', IntegerType()),\n",
    "                                StructField('cluster', IntegerType())])\n",
    "\n",
    "station_id_df = sqlContext.createDataFrame(station_id_rd, station_id_schema)\n",
    "\n",
    "\n",
    "#import math\n",
    "#station_final_cluster = station_cluster.withColumn('cluster').drop('avg_cluster')\n",
    "\n",
    "# station_final_cluster = sqlContext.createDataFrame(station_id_df.toPandas().groupby('station_id').\\\n",
    "#                                                    agg(lambda x:x.value_counts().index[0]).reset_index())\n",
    "\n",
    "station_final_cluster = sqlContext.createDataFrame(station_id_df.toPandas().groupby('station_id').\\\n",
    "                                                   median().reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({8: 64, 10: 5})"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([x.asDict()['cluster'] for x in station_final_cluster.select('cluster').collect()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|station_id|cluster|\n",
      "+----------+-------+\n",
      "|        31|     10|\n",
      "|        32|     10|\n",
      "|        80|     10|\n",
      "|        82|     10|\n",
      "|        83|     10|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "station_final_cluster.filter('cluster == 10').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Normaliser example\n",
    "\n",
    "# y = sc.parallelize(range(1,11)).map(lambda x: (x, x**2))\n",
    "\n",
    "# y_df = sqlContext.createDataFrame(y, StructType([StructField('num', IntegerType()),\n",
    "#                                                  StructField('num_sq', IntegerType())]))\n",
    "\n",
    "# from pyspark.ml.feature import Normalizer\n",
    "\n",
    "# va_small = VectorAssembler(inputCols=['num', 'num_sq'],\n",
    "#                            outputCol='features')\n",
    "\n",
    "# y_df_transformed = va_small.transform(y_df).select('features')\n",
    "# y_df_transformed.show(5)\n",
    "\n",
    "# norm = Normalizer(inputCol = 'features', outputCol = 'scaledFeatures')\n",
    "\n",
    "# # Compute summary statistics by fitting the StandardScaler\n",
    "# normData = norm.transform(y_df_transformed).select('scaledFeatures').\\\n",
    "#                 withColumnRenamed('scaledFeatures', 'features')\n",
    "\n",
    "# normData.collect()\n",
    "\n",
    "# num = y_df.select('num').collect()\n",
    "# y_df = y_df.drop('num')\n",
    "\n",
    "# num = [x.asDict()['num'] for x in num]\n",
    "\n",
    "# y_pd = y_df.toPandas()\n",
    "# y_pd['num'] = num\n",
    "\n",
    "# y_df_num = sqlContext.createDataFrame(y_pd)\n",
    "\n",
    "# y_df_num.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
