{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "\n",
    "INPUT_DATA = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "station_id,bikes_available,docks_available,time\r\n",
      "2,2,25,2013/08/29 12:06:01\r\n",
      "2,2,25,2013/08/29 12:07:01\r\n",
      "2,2,25,2013/08/29 12:08:01\r\n",
      "2,2,25,2013/08/29 12:09:01\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 {INPUT_DATA}status.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data from Mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- bikes_available: integer (nullable = true)\n",
      " |-- docks_available: integer (nullable = true)\n",
      " |-- station_id: integer (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "status_df = sqlContext.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    ".option(\"uri\",\"mongodb://54.245.37.88:27017/bikeshare.status\").load()\n",
    "status_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#functions to transform dataframe\n",
    "\n",
    "def toIntSafe(num):\n",
    "    try:\n",
    "        return int(num)\n",
    "    except ValueError:\n",
    "        return None\n",
    "    \n",
    "def toFloatSafe(num):\n",
    "    try:\n",
    "        return float(num)\n",
    "    except ValueError:\n",
    "        return None    \n",
    "    \n",
    "def toTimeStampSafe(data):\n",
    "    try:\n",
    "        if '/' in data:\n",
    "            return datetime.strptime(data, \"%Y/%m/%d %H:%M:%S\") \n",
    "        elif '-' in data:\n",
    "            return datetime.strptime(data, \"%Y-%m-%d %H:%M:%S\") \n",
    "    except ValueError:\n",
    "        return None\n",
    "    \n",
    "def get_time_of_day(x):\n",
    "    x = toTimeStampSafe(x)\n",
    "    if x:\n",
    "        hour = x.hour\n",
    "        if hour >= 6 and hour < 12:\n",
    "            return 'Morning'\n",
    "        if hour >= 12 and hour < 16:\n",
    "            return 'Afternoon'\n",
    "        if hour >= 16 and hour < 20:\n",
    "            return 'Evening'\n",
    "        if hour >= 20 or hour < 6:\n",
    "            return 'Night'\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def get_day_of_week(x):\n",
    "    x = toTimeStampSafe(x)\n",
    "    if x:\n",
    "        return x.isoweekday()\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def get_month(x):\n",
    "    x = toTimeStampSafe(x)\n",
    "    if x:\n",
    "        return x.month\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "timefunction = udf(lambda x: get_time_of_day(x))\n",
    "weekfunction = udf(lambda x: get_day_of_week(x), IntegerType())\n",
    "monthfunction = udf(lambda x: get_month(x), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "status_df = status_df.withColumn('bike_util', \\\n",
    "        ((col('docks_available') * 1.0) /(col('bikes_available') + col('docks_available'))))\n",
    "\n",
    "status = status_df.select('station_id', 'bike_util', timefunction('time').alias('day_part'), \\\n",
    "                 weekfunction('time').alias('day_of_week'), monthfunction('time').alias('month')).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+---------+-----------+-----+\n",
      "|station_id|         bike_util| day_part|day_of_week|month|\n",
      "+----------+------------------+---------+-----------+-----+\n",
      "|         2|0.9259259259259259|Afternoon|          4|    8|\n",
      "|         2|0.9259259259259259|Afternoon|          4|    8|\n",
      "|         2|0.9259259259259259|Afternoon|          4|    8|\n",
      "|         2|0.9259259259259259|Afternoon|          4|    8|\n",
      "|         2|0.9259259259259259|Afternoon|          4|    8|\n",
      "+----------+------------------+---------+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "status.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71984434"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_id: integer (nullable = true)\n",
      " |-- bike_util: double (nullable = true)\n",
      " |-- day_part: string (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "status.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status.select('day_of_week').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status.select('month').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status.select('day_part').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------+-----------+--------------------+------------------+\n",
      "|station_id|month|day_part|day_of_week|       var_util_perc|     avg_util_perc|\n",
      "+----------+-----+--------+-----------+--------------------+------------------+\n",
      "|         5|   12| Evening|          6| 0.01589297288090089|0.6118421052631583|\n",
      "|        34|   12| Evening|          1|0.021263267644319888| 0.573987154150196|\n",
      "|        38|   12| Evening|          7|0.005679260136055975| 0.548456790123464|\n",
      "|        41|   11| Evening|          7|0.005722599626027153|0.5439814814814791|\n",
      "|        45|    8|   Night|          5|0.023555367191730767|0.7002754820936453|\n",
      "+----------+-----+--------+-----------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bike_status_final = status.groupBy('station_id', 'month', 'day_part', 'day_of_week')\\\n",
    "                        .agg(F.variance('bike_util').alias('var_util_perc'),\n",
    "                             F.mean('bike_util').alias('avg_util_perc'))\n",
    "bike_status_final.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23520"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike_status_final.cache()\n",
    "bike_status_final.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running K Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting categorical columns to numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def implement_string_indexer(cols, df):\n",
    "    for c in cols:\n",
    "        si = StringIndexer(inputCol=c, outputCol=c+'_si')\n",
    "        sm = si.fit(df)\n",
    "        df = sm.transform(df).drop(c)\n",
    "        df = df.withColumnRenamed(c + '_si', c)\n",
    "        return df\n",
    "\n",
    "cols = ['day_part']\n",
    "\n",
    "final_df = implement_string_indexer(cols, bike_status_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------+--------------------+------------------+--------+\n",
      "|station_id|month|day_of_week|       var_util_perc|     avg_util_perc|day_part|\n",
      "+----------+-----+-----------+--------------------+------------------+--------+\n",
      "|         5|   12|          6| 0.01589297288090089|0.6118421052631583|     0.0|\n",
      "|        34|   12|          1|0.021263267644319888| 0.573987154150196|     0.0|\n",
      "|        38|   12|          7|0.005679260136055975| 0.548456790123464|     0.0|\n",
      "|        41|   11|          7|0.005722599626027153|0.5439814814814791|     0.0|\n",
      "|        45|    8|          5|0.023555367191730767|0.7002754820936453|     1.0|\n",
      "+----------+-----+-----------+--------------------+------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.cache()\n",
    "final_df.count()\n",
    "final_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing station_id from input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "station_ids = final_df.select('station_id').collect()\n",
    "station_ids = [x.asDict()['station_id'] for x in station_ids]\n",
    "df_without_sid = final_df.drop('station_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting input_data to required_format for K Means i.e getting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[6.0,0.6118421052...|\n",
      "|[1.0,0.5739871541...|\n",
      "|[7.0,0.5484567901...|\n",
      "|[7.0,0.5439814814...|\n",
      "|[5.0,0.7002754820...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_cols = ['day_of_week', 'avg_util_perc', 'month', \n",
    "              'day_part', 'var_util_perc']\n",
    "\n",
    "va = VectorAssembler(inputCols= input_cols, outputCol= 'features')\n",
    "df_transformed = va.transform(df_without_sid).select('features')\n",
    "df_transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------+\n",
      "|features                                              |\n",
      "+------------------------------------------------------+\n",
      "|[6.0,0.6118421052631583,12.0,0.0,0.01589297288090089] |\n",
      "|[1.0,0.573987154150196,12.0,0.0,0.021263267644319888] |\n",
      "|[7.0,0.548456790123464,12.0,0.0,0.005679260136055975] |\n",
      "|[7.0,0.5439814814814791,11.0,0.0,0.005722599626027153]|\n",
      "|[5.0,0.7002754820936453,8.0,1.0,0.023555367191730767] |\n",
      "+------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transformed.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23520"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transformed.cache()\n",
    "df_transformed.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalising Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23520"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=True)\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel = scaler.fit(df_transformed)\n",
    "\n",
    "# Normalize each feature to have unit standard deviation.\n",
    "scaledData = scalerModel.transform(df_transformed).select('scaledFeatures').\\\n",
    "                withColumnRenamed('scaledFeatures', 'features')\n",
    "    \n",
    "scaledData.cache()\n",
    "scaledData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_wssse = 1e10\n",
    "final_data = scaledData\n",
    "\n",
    "for clusters in range(2,4):\n",
    "\n",
    "    # Trains a k-means model.\n",
    "    kmeans = KMeans().setK(clusters).setSeed(1)\n",
    "    model = kmeans.fit(final_data)\n",
    "\n",
    "    # Evaluate clustering by computing Within Set Sum of Squared Errors.\n",
    "    wssse = model.computeCost(final_data)\n",
    "    if wssse <= min_wssse:\n",
    "        min_wssse = wssse\n",
    "        best_n_clusters = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Within Set Sum of Squared Errors = 86945.6121498\n",
      "Best number of clusters = 3\n",
      "Cluster Centers: \n",
      "[ 0.13371221 -0.20714931  0.91652861  0.07345454 -0.39622161]\n",
      "[-0.2390639  -0.06985938  0.00163782 -0.2316089   1.44021768]\n",
      "[ 0.00832689  0.23807379 -0.87807141  0.06174463 -0.44188773]\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Within Set Sum of Squared Errors = \" + str(min_wssse))\n",
    "print(\"Best number of clusters = \" + str(best_n_clusters))\n",
    "\n",
    "kmeans = KMeans().setK(best_n_clusters).setSeed(1)\n",
    "model = kmeans.fit(final_data)\n",
    "\n",
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.53489137e+00  -1.45322870e-02   9.40956186e+00   1.22823957e+00\n",
      "   5.35692946e-03]\n",
      "[ 1.04378693  0.05774723  3.46277178  0.7706444   0.05748734]\n",
      "[ 2.03335007  0.21986591 -2.25533824  1.2106747   0.00406062]\n"
     ]
    }
   ],
   "source": [
    "for center in centers:\n",
    "    print(center*scalerModel.mean + scalerModel.std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = model.transform(final_data).select('prediction').collect()\n",
    "predicted_cluster = [x.asDict()['prediction'] for x in prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 8910, 1: 5296, 2: 9314})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(predicted_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "station_id_rd =  sc.parallelize(zip(station_ids, predicted_cluster))\n",
    "\n",
    "station_id_schema = StructType([StructField('station_id', IntegerType()),\n",
    "                                StructField('cluster', IntegerType())])\n",
    "\n",
    "station_id_df = sqlContext.createDataFrame(station_id_rd, station_id_schema)\n",
    "\n",
    "\n",
    "station_final_cluster = sqlContext.createDataFrame(station_id_df.toPandas().groupby('station_id').\\\n",
    "                                                   agg(lambda x:x.value_counts().index[0]).reset_index())\n",
    "\n",
    "# station_final_cluster = sqlContext.createDataFrame(station_id_df.toPandas().groupby('station_id').\\\n",
    "#                                                    median().reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 15, 1: 20, 2: 35})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([x.asDict()['cluster'] for x in station_final_cluster.select('cluster').collect()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|cluster|stations                                                                                                                               |\n",
      "+-------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0      |[3, 7, 8, 11, 12, 23, 25, 26, 30, 35, 36, 38, 46, 63, 84]                                                                              |\n",
      "|1      |[39, 48, 50, 54, 55, 57, 60, 61, 64, 65, 66, 68, 69, 70, 71, 72, 73, 74, 76, 77]                                                       |\n",
      "|2      |[2, 4, 5, 6, 9, 10, 13, 14, 16, 21, 22, 24, 27, 28, 29, 31, 32, 33, 34, 37, 41, 42, 45, 47, 49, 51, 56, 58, 59, 62, 67, 75, 80, 82, 83]|\n",
      "+-------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_clusters = station_final_cluster.groupBy('cluster').\\\n",
    "                    agg(F.sort_array(F.collect_list('station_id')).alias('stations'))\n",
    "\n",
    "final_clusters.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(station_id=2, cluster=2),\n",
       " Row(station_id=3, cluster=0),\n",
       " Row(station_id=4, cluster=2),\n",
       " Row(station_id=5, cluster=2),\n",
       " Row(station_id=6, cluster=2),\n",
       " Row(station_id=7, cluster=0),\n",
       " Row(station_id=8, cluster=0),\n",
       " Row(station_id=9, cluster=2),\n",
       " Row(station_id=10, cluster=2),\n",
       " Row(station_id=11, cluster=0),\n",
       " Row(station_id=12, cluster=0),\n",
       " Row(station_id=13, cluster=2),\n",
       " Row(station_id=14, cluster=2),\n",
       " Row(station_id=16, cluster=2),\n",
       " Row(station_id=21, cluster=2),\n",
       " Row(station_id=22, cluster=2),\n",
       " Row(station_id=23, cluster=0),\n",
       " Row(station_id=24, cluster=2),\n",
       " Row(station_id=25, cluster=0),\n",
       " Row(station_id=26, cluster=0),\n",
       " Row(station_id=27, cluster=2),\n",
       " Row(station_id=28, cluster=2),\n",
       " Row(station_id=29, cluster=2),\n",
       " Row(station_id=30, cluster=0),\n",
       " Row(station_id=31, cluster=2),\n",
       " Row(station_id=32, cluster=2),\n",
       " Row(station_id=33, cluster=2),\n",
       " Row(station_id=34, cluster=2),\n",
       " Row(station_id=35, cluster=0),\n",
       " Row(station_id=36, cluster=0),\n",
       " Row(station_id=37, cluster=2),\n",
       " Row(station_id=38, cluster=0),\n",
       " Row(station_id=39, cluster=1),\n",
       " Row(station_id=41, cluster=2),\n",
       " Row(station_id=42, cluster=2),\n",
       " Row(station_id=45, cluster=2),\n",
       " Row(station_id=46, cluster=0),\n",
       " Row(station_id=47, cluster=2),\n",
       " Row(station_id=48, cluster=1),\n",
       " Row(station_id=49, cluster=2),\n",
       " Row(station_id=50, cluster=1),\n",
       " Row(station_id=51, cluster=2),\n",
       " Row(station_id=54, cluster=1),\n",
       " Row(station_id=55, cluster=1),\n",
       " Row(station_id=56, cluster=2),\n",
       " Row(station_id=57, cluster=1),\n",
       " Row(station_id=58, cluster=2),\n",
       " Row(station_id=59, cluster=2),\n",
       " Row(station_id=60, cluster=1),\n",
       " Row(station_id=61, cluster=1),\n",
       " Row(station_id=62, cluster=2),\n",
       " Row(station_id=63, cluster=0),\n",
       " Row(station_id=64, cluster=1),\n",
       " Row(station_id=65, cluster=1),\n",
       " Row(station_id=66, cluster=1),\n",
       " Row(station_id=67, cluster=2),\n",
       " Row(station_id=68, cluster=1),\n",
       " Row(station_id=69, cluster=1),\n",
       " Row(station_id=70, cluster=1),\n",
       " Row(station_id=71, cluster=1),\n",
       " Row(station_id=72, cluster=1),\n",
       " Row(station_id=73, cluster=1),\n",
       " Row(station_id=74, cluster=1),\n",
       " Row(station_id=75, cluster=2),\n",
       " Row(station_id=76, cluster=1),\n",
       " Row(station_id=77, cluster=1),\n",
       " Row(station_id=80, cluster=2),\n",
       " Row(station_id=82, cluster=2),\n",
       " Row(station_id=83, cluster=2),\n",
       " Row(station_id=84, cluster=0)]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "station_final_cluster.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_clusters.toPandas().to_csv(INPUT_DATA + 'final_clusters.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster,stations\r\n",
      "0,\"[3, 7, 8, 11, 12, 23, 25, 26, 30, 35, 36, 38, 46, 63, 84]\"\r\n",
      "1,\"[39, 48, 50, 54, 55, 57, 60, 61, 64, 65, 66, 68, 69, 70, 71, 72, 73, 74, 76, 77]\"\r\n",
      "2,\"[2, 4, 5, 6, 9, 10, 13, 14, 16, 21, 22, 24, 27, 28, 29, 31, 32, 33, 34, 37, 41, 42, 45, 47, 49, 51, 56, 58, 59, 62, 67, 75, 80, 82, 83]\"\r\n"
     ]
    }
   ],
   "source": [
    "!cat {INPUT_DATA}final_clusters.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_cluster = dict([(x['station_id'], x['cluster']) for x in station_final_cluster.collect()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cluster = udf(lambda x: station_cluster[x])\n",
    "\n",
    "status_cluster = status.withColumn('cluster', get_cluster(final_df['station_id']))\n",
    "\n",
    "status_cluster = status_cluster.drop('month', 'day_of_week', 'day_part', 'station_id')\n",
    "\n",
    "status_cluster.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71984434"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status_cluster.cache()\n",
    "status_cluster.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+\n",
      "|         bike_util|cluster|\n",
      "+------------------+-------+\n",
      "|0.9259259259259259|      2|\n",
      "|0.9259259259259259|      2|\n",
      "|0.9259259259259259|      2|\n",
      "+------------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "status_cluster.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+\n",
      "|cluster|       var_util_perc|     avg_util_perc|\n",
      "+-------+--------------------+------------------+\n",
      "|      0|0.027781789432157228|0.4870160515236175|\n",
      "|      1| 0.05256571108005761|0.4930798261639537|\n",
      "|      2|0.030443411074540416|0.5579522346854525|\n",
      "+-------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "status_cluster.groupBy('cluster').agg(F.variance('bike_util').alias('var_util_perc'),\n",
    "                             F.mean('bike_util').alias('avg_util_perc')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
